This is the **foundation** of Kubernetes. Let’s break down the **core components & architecture** step-by-step so you can *really* understand how everything fits together.

---

## 🧩 **Kubernetes Architecture Overview**

Kubernetes follows a **Master–Worker (Control Plane–Node)** model.

There are **two main parts:**

1. 🧠 **Control Plane (Master Node)** → Makes global decisions about the cluster (scheduling, scaling, health).
2. 💪 **Worker Nodes** → Actually run your application containers.

Here’s a simple flow diagram (conceptually):

```
            +-------------------+
            |   kubectl / API   |
            +---------+---------+
                      |
              [Control Plane]
+--------------------------------------------------------+
|  +-------------------+    +--------------------------+ |
|  | kube-apiserver    |    | etcd (key-value store)   | |
|  +-------------------+    +--------------------------+ |
|  | controller-manager|    | scheduler                | |
|  +-------------------+    +--------------------------+ |
+--------------------------------------------------------+
                      |
              [Worker Nodes]
+--------------------------------------------------------+
| +------------+ +----------------+ +------------------+ |
| | kubelet    | | kube-proxy     | | Container Runtime| |
| +------------+ +----------------+ +------------------+ |
|           (Runs your Pods and Containers)              |
+--------------------------------------------------------+
```

---

## ⚙️ **1. API Server (`kube-apiserver`)**

**The brain of Kubernetes communication.**

* Acts as the **entry point** for all Kubernetes commands.
* Every request (from `kubectl`, controllers, schedulers, etc.) goes through it.
* It **authenticates**, **validates**, and **updates state** in `etcd`.

📘 Example:
When you run `kubectl create pod nginx`, the API server:

1. Validates your YAML.
2. Stores it in etcd.
3. Notifies the scheduler to place it on a node.

---

## 🧱 **2. etcd**

**A distributed key–value store** that keeps all cluster data.

* Stores the **desired state** of the cluster (e.g., how many pods should exist).
* Highly available and consistent.
* The **single source of truth** for Kubernetes.

💡 Think of `etcd` as Kubernetes’ database.

📘 Example:
If you create a deployment with 3 replicas, that info is stored in `etcd`.
If a pod crashes, Kubernetes checks etcd → sees 3 replicas are desired → starts a new pod.

---

## 🤖 **3. Controller Manager**

**Maintains the desired state** of the system.

* Continuously watches the cluster through the API server.
* Ensures that what’s running = what’s declared in etcd.
* Runs multiple controllers inside:

  * **Node controller** → checks node health.
  * **Replication controller** → maintains the correct number of pods.
  * **Endpoint controller** → manages service endpoints.
  * **ServiceAccount & Token controller** → manages access tokens.

📘 Example:
If a pod dies, the ReplicationController creates a new one automatically.

---

## 🎯 **4. Scheduler**

**Decides where (which node)** a Pod should run.

* Watches for newly created pods that don’t have a node assigned.
* Chooses the best node based on:

  * Resource requests (CPU/memory)
  * Node labels / taints
  * Affinity/anti-affinity rules

📘 Example:
When you create a pod, the Scheduler picks a node and tells the kubelet to start it.

---

## 🧍 **5. Kubelet (on Worker Node)**

**The agent running on each node.**

* Talks to the API server.
* Makes sure containers are running as defined in the PodSpec.
* Reports node & pod status back to the control plane.
* Uses **Container Runtime Interface (CRI)** (like containerd or Docker) to start/stop containers.

📘 Example:
When the Scheduler assigns a pod to Node-1, the Kubelet on Node-1 pulls the container image and runs it.

---

## 🔄 **6. Kube-Proxy**

**Handles networking on each node.**

* Manages network routing for pods and services.
* Ensures that traffic from inside/outside the cluster reaches the correct Pod.
* Implements **Kubernetes Service networking** using iptables or IPVS rules.

📘 Example:
When you expose a deployment as a Service, kube-proxy routes incoming requests to one of the backend pods.

---

## 🧱 **7. Container Runtime**

**Runs the containers inside pods.**

Supported runtimes:

* containerd (default)
* CRI-O
* Docker (deprecated but still usable)

The kubelet communicates with this runtime using the **Container Runtime Interface (CRI)**.

---

## 🧠 **How it All Works Together**

1. You apply a YAML file with `kubectl apply -f app.yaml`.
2. The API Server validates and stores it in etcd.
3. The Scheduler finds a node.
4. The Kubelet on that node starts the containers.
5. The Controller Manager ensures the right number of pods exist.
6. The Kube-Proxy handles the networking.

---

## 🔍 **Quick Summary Table**

| Component              | Location      | Purpose                                 |
| ---------------------- | ------------- | --------------------------------------- |
| **kube-apiserver**     | Control Plane | Entry point for all commands & REST API |
| **etcd**               | Control Plane | Stores cluster state                    |
| **controller-manager** | Control Plane | Ensures cluster’s desired state         |
| **scheduler**          | Control Plane | Assigns pods to nodes                   |
| **kubelet**            | Worker Node   | Runs and manages containers             |
| **kube-proxy**         | Worker Node   | Manages networking and service routing  |
| **container runtime**  | Worker Node   | Runs containers (containerd/CRI-O)      |

---

## 🧰 Hands-on Practice

Try this to see it in action 👇

```bash
# See control plane components (if using kind/minikube)
kubectl get pods -n kube-system

# Describe one component
kubectl describe pod -n kube-system kube-apiserver-<your-master-node>

# Check where etcd stores data
kubectl logs -n kube-system etcd-<your-master-node>
```
